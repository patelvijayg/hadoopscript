hdfs version
hdfs classpath
hdfs dfs -ls /
hdfs dfsadmin -report
hdfs dfsadmin -refreshNodes
hdfs getconf -namenodes
hdfs getconf -secondaryNameNodes
hdfs fsck / -files
hdfs fsck / -openforwrite
hdfs fsck /file -files -blocks -locations
hadoop fs -Ddfs.replication=2 -put file /hadoopdir
hdfs getconf -confKey dfs.heartbeat.interval
$SPARK_HOME/sbin/stop-all.sh && slepp 4 && copyandsync
$SPARK_HOME/bin/spark-submit $SPARK_HOME/examples/src/main/python/pi.py  1000 --conf spark.executor.memory=64m  --conf spark.executor.cores=1 --master yarn --deploy-mode client
ssh -t slave1  "~/scripts/node.sh start"

 ./yarn-daemon.sh start nodemanager
 ./hadoop-daemon.sh start datanode

dead node criteria
 dfs.heartbeat.interval 5
 dfs.namenode.stale.datanode.interval	50
update replica
hdfs dfs -setrep -R -w 2 /data1/uPM_all_metrics-20180730080000-20180730081500.csv

tail -f  yarn-vagrant-nodemanager-slave1.log
sudo tcpdump -i any udp port 8020 -vvv -w /path/file.pcap
yarn node -list

from pyspark.sql import SparkSession
sparkSession = SparkSession.builder.appName("firstapp").getOrCreate()
df_load = sparkSession.read.csv('hdfs://master1/data1/uPM_all_metrics-20180730080000-20180730081500.csv')
df_load.show()


    1  sudo /home/vagrant/extras/scripts/install_java_hadoop_spark.sh 
    2  /home/vagrant/scripts/setup_variable.sh
    3  history 
    4  /home/vagrant/scripts/copy_files.sh 
    5  cat /usr/hadoop/etc/hadoop/core-site.xml 
    6  history 
    7  cd /home/vagrant/scripts/
    8  ls
    9  cat order_script.sh 
   10  ./clean_master.sh
   11  hdstartall 
   12  echo $SPARK_HOME 
   13  cat ~/.bashrc 
   14  sed -e 's/\r$//' /home/vagrant/scripts/hadoop_variables.txt
   15  sed -i -e 's/\r$//' /home/vagrant/scripts/hadoop_variables.txt
   16  exit
   17  hdstopall 
   18  type hdstopall
   19  hdstartall 
   20  start-yarn.sh 
   21  vi ~/scripts/alias.txt 
   22  . ~/scripts/alias.txt 
   23  hdstopall 
   24  hdstartall 
   25  cat ~/scripts/alias.txt
   26  ls
   27  cd scripts/
   28  ls
   29  ./master_only_rpm.sh 
   30  cd ..
   31  cd 
   32  ls
   33  cat somecommand.txt 
   34  7za x todetele.7z
   35  cat ~/scripts/master_only_rpm.sh 
   36  sudo yum -y install vim nano p7zip unzip net-tools tcpdump wget
   37  cat ~/scripts/master_only_rpm.sh 
   38  yum install epel-release -y
   39  sudo yum install epel-release -y
   40  sudo yum -y install vim nano p7zip unzip net-tools tcpdump wget
   41  ls
   42  cat somecommand.txt 
   43  7za x todetele.7z
   44  jps
   45  hdfs dfs -mkdir /data1
   46  hdfs dfs -put /home/vagrant/todetele/uPM_all_metrics-20180730080000-20180730081500.csv /data1[vagrant@master1 ~]$ 7za x todetele.7z
   47  hdfs dfs -put /home/vagrant/todetele/uPM_all_metrics-20180730080000-20180730081500.csv /data1
   48  hd -ls /data1
   49  type hd
   50  hd-ls /data1
   51  hd -ls /data1
   52  hd 
   53  type hd
   54  hdls /data1
   55  which hd
   56  vi /home/vagrant/scripts/alias.txt 
   57  source /home/vagrant/scripts/alias.txt
   58  hd -ls /data1
   59  type hd
   60  hdfs dfs -cat /data1/uPM_all_metrics*
   61  vi /home/vagrant/scripts/alias.txt 
   62  source /home/vagrant/scripts/alias.txt
   63  hd -ls
   64  hd 
   65  type hd
   66  vi /home/vagrant/scripts/alias.txt 
   67  source /home/vagrant/scripts/alias.txt
   68  hd1 -ls /data1
   69  type hd1
   70  vi /home/vagrant/scripts/alias.txt 
   71  source /home/vagrant/scripts/alias.txt
   72  xhd1 -ls /data1
   73  type xhd1 
   74  type -h
   75  history
   76  env
   77  env | grep xhd
   78  echo $xhd1
   79  compgen -a
   80  alias 
   81  l
   82  l.
   83  cat /home/vagrant/scripts/alias.txt 
   84  echo >  /home/vagrant/scripts/alias.txt 
   85  vi  /home/vagrant/scripts/alias.txt 
   86  source  /home/vagrant/scripts/alias.txt 
   87  alias 
   88  hd -ls /
   89  vi  /home/vagrant/scripts/alias.txt 
   90  source  /home/vagrant/scripts/alias.txt 
   91  hd -ls /
   92  alias 
   93  l
   94  cd scripts/
   95  ls
   96  vi alias.txt 
   97  . alias.txt 
   98  alias 
   99  unalias hd1
  100  alias 
  101  hd -ls /data1
  102  jps
  103  hdstopall 
  104  hdstartall 
  105  hdconf 
  106  ls
  107  cat yarn-site.xml 
  108  cat yarn-env.sh 
  109  ls
  110  hdstopall 
  111  ls
  112  vi yarn-site.xml 
  113  /home/vagrant/scripts/syncnode.sh 
  114  hdstartall 
  115  type spark-daemon.sh
  116  pyspark 
  117  sudo netstat -auntlp | grep 8020
  118  jp
  119  jps
  120  ps -ef | grep java
  121  hdstopall 
  122  ls
  123  vi core-site.xml 
  124  /home/vagrant/scripts/syncnode.sh 
  125  /home/vagrant/scripts/syncnode.sh 6
  126  hdstartall 
  127  netstat -autnl | grep 8020
  128  pyspark 
  129  sudo tcpdump -i any udp port 8020 -vvv 
  130  sudo tcpdump -i any tcp port 8020 -vvv 
  131  sudo tcpdump -i any tcp port 8020 -vvv -XXX
  132  ls
  133  cd
  134  ls
  135  cd todetele/
  136  sudo tcpdump -i any tcp port 8020 -vvv -XXX -w aaa.pcap
  137  printenv 
  138  yarn node -list
  139  sudo netstat -auntlp
  140  sudo netstat -untlp
  141  history 

  
  cd ~
7za x todetele.7z
cd todetele/
hdfs dfs -mkdir /data1
hdfs dfs -put /home/vagrant/todetele/uPM_all_metrics-20180730080000-20180730081500.csv /data1


tail -f  yarn-vagrant-nodemanager-slave1.log
sudo tcpdump -i any udp port 8020 -vvv -w /path/file.pcap
yarn node -list

from pyspark.sql import SparkSession
sparkSession = SparkSession.builder.appName("firstapp").getOrCreate()
df_load = sparkSession.read.csv('hdfs://master1/data1/uPM_all_metrics-20180730080000-20180730081500.csv')
df_load.show()

    1  cd scripts/
    2  pwd
    3  ls
    4  mkdir $HADOOP_HOME/logs
    5  ./copy_files.sh
    6  rm $HADOOP_HOME/logs
    7  rm -rf $HADOOP_HOME/logs
    8  ./copy_files.sh
    9  ls
   10  cd extras/
   11  ls
   12  cd scripts/
   13  ls
   14  sudo ./install_java_hadoop_spark.sh
   15  /home/vagrant/scripts/setup_variable.sh
   16  cd scripts/
   17  ls
   18  cat ./reset_dfs.sh
   19  alias
   20  ./reset_dfs.sh
   21  ls
   22  cat testfile.sh
   23  hdstartall
   24  ./testfile.sh
   25  hdconf
   26  ls
   27  vi core-site.xml
   28  scritpdir
   29  hdstopall
   30  ./reset_dfs.sh
   31  ./syncnode.sh
   32  hdstartall
   33  vi ./cleanhdp.sh
   34  ls
   35  cat syncnode.sh
   36  cat reset_dfs.sh
   37  cat ./cleanhdp_slave.sh
   38  hdstopall
   39  ./cleanhdp_slave.sh
   40  which java
   41  jps
   42  hdstartall
   43  ls
   44  ./testfile.sh
   45  hdconf
   46  vi hdfs-site.xml
   47  type hdstopall
   48  stop-yarn.sh
   49  start-yarn.sh
   50  stop-yarn.sh
   51  stop-dfs.sh
   52  scritpdir
   53  ls
   54  ./reset_dfs.sh
   55  ./syncnode.sh
   56  hdstartall
   57  ./testfile.sh
   58  hdfs dfsadmin -refreshNodes
   59  hdconf
   60  vi slaves
   61  hdfs dfsadmin -refreshNodes
   62  hdfs getconf -namenodes
   63  hdfs fsck / -files
   64  hadoop dfsadmin -refreshNodes
   65  hdfs dfsadmin -refreshNodes
   66  jps
   67  history
   68  cat ~/scripts/testfile.sh
   69  hadoop dfs -setrep -R -w 2 /data1/uPM_all_metrics-20180730080000-20180730081500.csv
   70  hdfs dfsadmin -refreshNodes
   71  hdfs dfsadmin -report
   72  ls
   73  vi hdfs-site.xml
   74  hdfs dfsadmin -refreshNodes
   75  hdfs dfsadmin -report
   76  hdfs dfsadmin -refreshNamenodes
   77  hdfs dfsadmin -refreshNodes
   78  hdfs dfsadmin -report
   79  hdfs getconf -namenodes
   80  hdfs getconf -confKey dfs.heartbeat.interval
   81  hdfs getconf -confKey dfs.namenode.stale.datanode.interval
   82  hdfs storagepolicies
   83  scritpdir
   84  ls
   85  cat syncnode.sh
   86  vi node.sh
   87  chmod +xw node.sh
   88  ./node.sh stop
   89  vi node.sh
   90  ./node.sh stop
   91  vi node.sh
   92  ./node.sh stop
   93  cat node.sh
   94  ./syncnode.sh
   95  hdfs dfsadmin -refreshNodes
   96  ssh -t slave3 -c "~/scripts/node.sh start"
   97  ssh -t slave3  "~/scripts/node.sh start"
   98  ssh -t slave1  "~/scripts/node.sh start"
   99  ssh -t slave1  "~/scripts/node.sh stop"
  100  ssh -t slave3  "~/scripts/node.sh start"
  101  hdfs dfsadmin -report
  102  hdconf
  103  vi hdfs-site.xml
  104  hdfs dfsadmin -refreshNodes
  105  scritpdir
  106  ./syncnode.sh
  107  hdfs dfsadmin -refreshNodes
  108  ssh -t slave1  "~/scripts/node.sh start"
  109  ssh -t slave1  "~/scripts/node.sh stop"
  110  history
  111  hdfs getconf -confKey dfs.namenode.heartbeat.recheck-interval
  112  hdfs dfsadmin -report
  113  hdfs getconf -confKey dfs.namenode.stale.datanode.interval
  114  hdfs dfsadmin -report
  115  history
